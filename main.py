#pip install openai==0.28
#pip install pinecone-client
import openai
import pinecone
import streamlit as st
from datetime import datetime

# NastavenÃ­ pÅ™Ã­stupovÃ½ch klÃ­ÄÅ¯ pomocÃ­ Streamlit secrets
openai.api_key = st.secrets["OPENAI_API_KEY"]

# Inicializace Pinecone
pinecone.init(
    api_key=st.secrets["PINECONE_API_KEY"],
    environment="us-east-1-aws"  #
)
index = pc.Index("fischatbot")

def get_embedding(text):
    response = openai.Embedding.create(input=text, model="text-embedding-ada-002")
    return response['data'][0]['embedding']

def save_unanswered_query(query, filename="questions_for_processing.txt"):
    """UklÃ¡dÃ¡ otÃ¡zky bez relevantnÃ­ch vÃ½sledkÅ¯ do souboru spoleÄnÄ› s datem."""
    with open(filename, "a", encoding="utf-8") as file:
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        file.write(f"OtÃ¡zka: {query}\nÄŒas: {current_time}\n")
        file.write("\n" + "-" * 50 + "\n\n")

def save_no_results_to_file(query, response="OmlouvÃ¡m se, ale nejsou k dispozici Å¾Ã¡dnÃ¡ relevantnÃ­ data k vaÅ¡emu dotazu.", filename="outputs.txt"):
    """UklÃ¡dÃ¡ zÃ¡znamy bez relevantnÃ­ch vÃ½sledkÅ¯ do outputs.txt."""
    with open(filename, "a", encoding="utf-8") as file:
        file.write(f"PÅ¯vodnÃ­ uÅ¾ivatelskÃ½ dotaz: {query}\n")
        file.write(f"VygenerovanÃ¡ odpovÄ›Ä modelem: {response}\n")
        file.write(f"Verze experimentu: Final\n") ##--------------------------------------------zmÄ›nit
        file.write(f"MinimÃ¡lnÃ­ skÃ³re podobnosti: 0.82\n")  # PevnÄ› nastavenÃ© minimÃ¡lnÃ­ skÃ³re ---------
        file.write(f"MaximÃ¡lnÃ­ skÃ³re podobnosti: 0.90\n")  # PevnÄ› nastavenÃ© maximÃ¡lnÃ­ skÃ³re ---------
        file.write(f"SpotÅ™eba tokenÅ¯: 0\n")  # Å½Ã¡dnÃ¡ spotÅ™eba tokenÅ¯
        file.write(f"SpotÅ™eba tokenÅ¯ - VstupnÃ­ (prompt): 0\n")  # Å½Ã¡dnÃ¡ spotÅ™eba tokenÅ¯
        file.write(f"SpotÅ™eba tokenÅ¯ - VÃ½stupnÃ­ (completion): 0\n")  # Å½Ã¡dnÃ¡ spotÅ™eba tokenÅ¯
        file.write("\n" + "-" * 50 + "\n\n")  # OddÄ›lenÃ­ jednotlivÃ½ch dotazÅ¯

def retrieve_similar_texts(query, top_k=2): 
    try:
        query_embedding = get_embedding(query)

        # Filtr pro vector_id, kterÃ© zaÄÃ­najÃ­ na 'text_query'
        result = index.query(
            vector=query_embedding,
            top_k=top_k,
            include_metadata=True,
            filter={
                "metadata_field_name": {"$eq": "text_query"}
            }
        )


        matches = []
        for match in result['matches']:
            score = match['score']
            if score >= 0.82:  # Filtrujeme pouze vektory s minimÃ¡lnÃ­m skÃ³re 0,82------------------------zmenit
                chunk_text = match['metadata'].get('chunk_text', 'No chunk text available')
                matches.append({'score': score, 'chunk_text': chunk_text})

        # Pokud nejsou Å¾Ã¡dnÃ© relevantnÃ­ vÃ½sledky, uloÅ¾Ã­me otÃ¡zku do souboru
        if not matches:
            save_unanswered_query(query)
            save_no_results_to_file(query)  # UloÅ¾Ã­me i do outputs.txt
        
        return matches
    except Exception as e:
        st.error(f"Chyba pÅ™i vyhledÃ¡vÃ¡nÃ­ textÅ¯: {e}")
        return []


# Funkce pro uklÃ¡dÃ¡nÃ­ vÃ½sledkÅ¯ do souboru, vÄetnÄ› generovanÃ© odpovÄ›di a spotÅ™eby tokenÅ¯
def save_results_to_file(query, response, min_score, max_score, token_usage, prompt_tokens, completion_tokens, context="", filename="outputs.txt"):
    with open(filename, "a", encoding="utf-8") as file:
        # ZÃ¡pis informacÃ­ o dotazu a odpovÄ›di
        file.write(f"PÅ¯vodnÃ­ uÅ¾ivatelskÃ½ dotaz: {query}\n")
        file.write(f"VygenerovanÃ¡ odpovÄ›Ä modelem: {response}\n")
        file.write(f"Verze experimentu: 3a\n")  ###------------------------------------------------------zmÄ›nit
        file.write(f"MinimÃ¡lnÃ­ skÃ³re podobnosti: {min_score}\n")
        file.write(f"MaximÃ¡lnÃ­ skÃ³re podobnosti: {max_score}\n")
        file.write(f"SpotÅ™eba tokenÅ¯: {token_usage}\n")
        file.write(f"SpotÅ™eba tokenÅ¯ - VstupnÃ­ (prompt): {prompt_tokens}\n")
        file.write(f"SpotÅ™eba tokenÅ¯ - VÃ½stupnÃ­ (completion): {completion_tokens}\n")
        file.write("PouÅ¾itÃ½ kontext pro generovÃ¡nÃ­ odpovÄ›di:\n")
        file.write(context + "\n")
        file.write("\n" + "-" * 50 + "\n\n")  # OddÄ›lenÃ­ jednotlivÃ½ch dotazÅ¯



# GlobÃ¡lnÃ­ promÄ›nnÃ¡ pro uchovÃ¡nÃ­ historie dotazÅ¯ a odpovÄ›dÃ­
history = []  


# Inicializace historie pomocÃ­ session_state
if 'history' not in st.session_state:
    st.session_state.history = []  # Inicializace historie jako prÃ¡zdnÃ©ho seznamu pÅ™i prvnÃ­m bÄ›hu

def generate_response(query, retrieved_texts):
    if not retrieved_texts:
        return "OmlouvÃ¡m se, ale nejsou k dispozici Å¾Ã¡dnÃ¡ relevantnÃ­ data k vaÅ¡emu dotazu."

    # ZÃ­skÃ¡nÃ­ minimÃ¡lnÃ­ho a maximÃ¡lnÃ­ho skÃ³re z nalezenÃ½ch textÅ¯
    if retrieved_texts:
        min_score = min(match['score'] for match in retrieved_texts)
        max_score = max(match['score'] for match in retrieved_texts)
    else:
        min_score, max_score = None, None

    # SestavenÃ­ kontextu z nalezenÃ½ch textÅ¯
    context = "\n\n".join([
        f"Text: {match['chunk_text']}"
        for match in retrieved_texts if 'chunk_text' in match
    ])

    # SestavenÃ­ historie pomocÃ­ seznamu 'messages'
    messages = [
        {
            "role": "system",
            "content": (
                "You are a helpful assistant specializing in providing information to prospective students of the Faculty of Informatics and Statistics (FIS). "
                "Your responses should be based exclusively on the provided FIS materials."
            )
        }
    ]

    # PÅ™idÃ¡nÃ­ minulÃ½ch dotazÅ¯ a odpovÄ›dÃ­ do historie
    for i, (q, a) in enumerate(st.session_state.history, 1):
        combined_content = f"Dotaz Ä. {i}: {q} | OdpovÄ›Ä Ä. {i}: {a}"
        messages.append({"role": "assistant", "content": combined_content})

    # PÅ™idÃ¡nÃ­ aktuÃ¡lnÃ­ho kontextu a dotazu
    messages.append({"role": "user", "content": f"NÃ¡sledujÃ­cÃ­ texty jsou relevantnÃ­ k dotazu:\n\n{context}\n\nOtÃ¡zka: {query}\nOdpovÄ›Ä:"})

    # >>>> ZDE BYL ODSTRANÄšN expander pro ladÄ›nÃ­ <<<<

    try:
        # GenerovÃ¡nÃ­ odpovÄ›di pomocÃ­ OpenAI
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=messages,
            max_tokens=1000,
            temperature=0.5,
            top_p=0.7
        )

        # UloÅ¾enÃ­ odpovÄ›di a aktualizace historie
        answer = response['choices'][0]['message']['content'].strip()
        st.session_state.history.append((query, answer))

        # ZÃ­skÃ¡nÃ­ spotÅ™eby tokenÅ¯
        token_usage = response['usage']['total_tokens']
        prompt_tokens = response['usage']['prompt_tokens']
        completion_tokens = response['usage']['completion_tokens']

        # UloÅ¾enÃ­ relevantnÃ­ch vÃ½sledkÅ¯ do souboru
        save_results_to_file(
            query=query,
            response=answer,
            min_score=min_score,
            max_score=max_score,
            token_usage=token_usage,
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            context=context
        )


        return answer
    except Exception as e:
        st.error(f"Chyba pÅ™i generovÃ¡nÃ­ odpovÄ›di: {e}")
        return "OmlouvÃ¡m se, doÅ¡lo k chybÄ› pÅ™i generovÃ¡nÃ­ odpovÄ›di."


####################
def retrieve_and_respond(query, top_k=1):
    """
    PokusÃ­ se najÃ­t jeden embedding s filtrem pro `text_response` a minimÃ¡lnÃ­m skÃ³re 0.90.
    Pokud takovÃ½ embedding existuje, uloÅ¾Ã­ data do souboru a vrÃ¡tÃ­ chunk_text jako odpovÄ›Ä.
    Pokud ne, provede sekundÃ¡rnÃ­ vyhledÃ¡vÃ¡nÃ­ a generovÃ¡nÃ­ odpovÄ›di.
    """
    try:
        # VytvoÅ™enÃ­ embeddingu pro dotaz
        query_embedding = get_embedding(query)

        # PrimÃ¡rnÃ­ dotaz pro hledÃ¡nÃ­ embeddingu s filtrem "text_response"
        result = index.query(
            vector=query_embedding,
            top_k=top_k,
            include_metadata=True,
            filter={
                "metadata_field_name": {"$eq": "text_response"}
            }
        )

        # ZpracovÃ¡nÃ­ vÃ½sledkÅ¯
        matches = []
        for match in result.get('matches', []):
            score = match['score']
            if score >= 0.90:  # MinimÃ¡lnÃ­ skÃ³re 0.90-----------------------------------------------zmenit
                chunk_text = match['metadata'].get('chunk_text', 'No chunk text available')
                metadata_field_name = match['metadata'].get('metadata_field_name', 'N/A')
                matches.append({
                    "score": score,
                    "chunk_text": chunk_text,
                    "metadata_field_name": metadata_field_name
                })


        # Pokud je nalezen alespoÅˆ jeden relevantnÃ­ vÃ½sledek
        if matches:
            best_match = matches[0]
            response = best_match['chunk_text']

            # UloÅ¾enÃ­ vÃ½sledkÅ¯ do souboru
            save_results_to_file(
                query=query,
                response=response,
                min_score=best_match['score'],
                max_score=best_match['score'],
                token_usage=0,  # Å½Ã¡dnÃ¡ spotÅ™eba tokenÅ¯ pÅ™i pÅ™Ã­mÃ© odpovÄ›di
                prompt_tokens=0,
                completion_tokens=0
            )

            return response

        # Pokud nebyly nalezeny relevantnÃ­ vÃ½sledky, pÅ™echÃ¡zÃ­ k sekundÃ¡rnÃ­mu vyhledÃ¡vÃ¡nÃ­
        retrieved_texts = retrieve_similar_texts(query, top_k=2)

        # GenerovÃ¡nÃ­ odpovÄ›di pomocÃ­ sekundÃ¡rnÃ­ho vyhledÃ¡vÃ¡nÃ­
        return generate_response(query, retrieved_texts)

    except Exception as e:
        st.error(f"Chyba pÅ™i vyhledÃ¡vÃ¡nÃ­ a odpovÃ­dÃ¡nÃ­: {e}")
        return "OmlouvÃ¡m se, doÅ¡lo k chybÄ› pÅ™i zpracovÃ¡nÃ­ vaÅ¡eho dotazu."



####################


# Streamlit aplikace
st.title("TestovacÃ­ chatbot Fakulty informatiky a statistiky VÅ E v Praze")

st.write("""
Tento chatbot byl vytvoÅ™en v rÃ¡mci diplomovÃ© prÃ¡ce, kterÃ¡ se zamÄ›Å™uje na minimalizaci nÃ¡kladÅ¯ generativnÃ­ch dialogovÃ½ch systÃ©mÅ¯ pomocÃ­ pÅ™Ã­stupu Retrieval-Augmented Generation (RAG).

ğŸ¤– Chatbot je urÄen k odpovÃ­dÃ¡nÃ­ na dotazy tÃ½kajÃ­cÃ­ se informacÃ­ o studiu na FakultÄ› informatiky a statistiky VÅ E v Praze. 
Jeho znalosti jsou omezeny na pÅ™edem definovanÃ¡ tÃ©mata.

â— **Chatbot v tÃ©to demoverzi odpovÃ­dÃ¡ na otÃ¡zky v tÄ›chto oblastech:**

â€¢ ğŸ“ **StudijnÃ­ programy** â€“ bakalÃ¡Å™skÃ©, magisterskÃ©, doktorskÃ© a MBA  
â€¢ ğŸ“… **PoÅ¾adavky na pÅ™ijetÃ­** â€“ dokumenty, podmÃ­nky, termÃ­ny pÅ™ihlÃ¡Å¡ek  
â€¢ ğŸ“ **PÅ™ijÃ­macÃ­ Å™Ã­zenÃ­** â€“ prÅ¯bÄ›h zkouÅ¡ek, testy  
â€¢ ğŸŒ **ZahraniÄnÃ­ studenti** â€“ jazykovÃ© poÅ¾adavky, nostrifikace, vÃ­za  
â€¢ ğŸ’¸ **FinanÄnÃ­ zÃ¡leÅ¾itosti** â€“ Å¡kolnÃ©, stipendia a dalÅ¡Ã­ moÅ¾nosti podpory
""")

# Vstup uÅ¾ivatele
query = st.text_input("Zadejte dotaz a ovÄ›Å™te, jak si chatbot poradÃ­! ğŸ‘‡")


if query:
    with st.spinner("VyhledÃ¡vÃ¡nÃ­ relevantnÃ­ch textÅ¯..."):
        # NovÃ½ pÅ™Ã­stup s prioritnÃ­m vyhledÃ¡nÃ­m konkrÃ©tnÃ­ho embeddingu
        st.subheader("GenerovanÃ¡ odpovÄ›Ä:")
        response = retrieve_and_respond(query)  # PouÅ¾ijeme novou funkci
        st.write(response)
        
              

        








