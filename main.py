#pip install openai==0.28
#pip install pinecone-client
import openai
import pinecone
import streamlit as st
from datetime import datetime
import io

# NastavenÃ­ pÅ™Ã­stupovÃ½ch klÃ­ÄÅ¯ pomocÃ­ Streamlit secrets
openai.api_key = st.secrets["OPENAI_API_KEY"]

# Inicializace Pinecone
pinecone.init(
    api_key=st.secrets["PINECONE_API_KEY"]
)
index = pinecone.Index("fischatbot")

# Inicializace session_state promÄ›nnÃ½ch
if 'history' not in st.session_state:
    st.session_state.history = []
if 'log' not in st.session_state:
    st.session_state.log = ""
if 'unanswered_log' not in st.session_state:
    st.session_state.unanswered_log = ""

def get_embedding(text):
    response = openai.Embedding.create(input=text, model="text-embedding-ada-002")
    return response['data'][0]['embedding']

def save_no_results_to_file(query, response="OmlouvÃ¡m se, ale nejsou k dispozici Å¾Ã¡dnÃ¡ relevantnÃ­ data k vaÅ¡emu dotazu."):
    log_entry = (
        f"PÅ¯vodnÃ­ uÅ¾ivatelskÃ½ dotaz: {query}\n"
        f"VygenerovanÃ¡ odpovÄ›Ä modelem: {response}\n"
        f"Verze experimentu: Final\n"
        f"MinimÃ¡lnÃ­ skÃ³re podobnosti: 0.82\n"
        f"MaximÃ¡lnÃ­ skÃ³re podobnosti: 0.90\n"
        f"SpotÅ™eba tokenÅ¯: 0\n"
        f"SpotÅ™eba tokenÅ¯ - VstupnÃ­ (prompt): 0\n"
        f"SpotÅ™eba tokenÅ¯ - VÃ½stupnÃ­ (completion): 0\n"
        + "-" * 50 + "\n\n"
    )
    st.session_state.unanswered_log += log_entry

def save_results_to_file(query, response, min_score, max_score, token_usage, prompt_tokens, completion_tokens, context=""):
    log_entry = (
        f"PÅ¯vodnÃ­ uÅ¾ivatelskÃ½ dotaz: {query}\n"
        f"VygenerovanÃ¡ odpovÄ›Ä modelem: {response}\n"
        f"Verze experimentu: 3a\n"
        f"MinimÃ¡lnÃ­ skÃ³re podobnosti: {min_score}\n"
        f"MaximÃ¡lnÃ­ skÃ³re podobnosti: {max_score}\n"
        f"SpotÅ™eba tokenÅ¯: {token_usage}\n"
        f"SpotÅ™eba tokenÅ¯ - VstupnÃ­ (prompt): {prompt_tokens}\n"
        f"SpotÅ™eba tokenÅ¯ - VÃ½stupnÃ­ (completion): {completion_tokens}\n"
        f"PouÅ¾itÃ½ kontext pro generovÃ¡nÃ­ odpovÄ›di:\n{context}\n"
        + "-" * 50 + "\n\n"
    )
    st.session_state.log += log_entry

def retrieve_similar_texts(query, top_k=2): 
    try:
        query_embedding = get_embedding(query)
        result = index.query(
            vector=query_embedding,
            top_k=top_k,
            include_metadata=True,
            filter={"metadata_field_name": {"$eq": "text_query"}}
        )
        matches = []
        for match in result['matches']:
            score = match['score']
            if score >= 0.82:
                chunk_text = match['metadata'].get('chunk_text', 'No chunk text available')
                matches.append({'score': score, 'chunk_text': chunk_text})
        if not matches:
            save_no_results_to_file(query)
        return matches
    except Exception as e:
        st.error(f"Chyba pÅ™i vyhledÃ¡vÃ¡nÃ­ textÅ¯: {e}")
        return []

def generate_response(query, retrieved_texts):
    if not retrieved_texts:
        return "OmlouvÃ¡m se, ale nejsou k dispozici Å¾Ã¡dnÃ¡ relevantnÃ­ data k vaÅ¡emu dotazu."
    min_score = min(match['score'] for match in retrieved_texts)
    max_score = max(match['score'] for match in retrieved_texts)
    context = "\n\n".join([f"Text: {match['chunk_text']}" for match in retrieved_texts if 'chunk_text' in match])
    messages = [
        {
            "role": "system",
            "content": (
                "You are a helpful assistant specializing in providing information to prospective students of the Faculty of Informatics and Statistics (FIS). "
                "Your responses should be based exclusively on the provided FIS materials."
            )
        }
    ]
    for i, (q, a) in enumerate(st.session_state.history, 1):
        combined_content = f"Dotaz Ä. {i}: {q} | OdpovÄ›Ä Ä. {i}: {a}"
        messages.append({"role": "assistant", "content": combined_content})
    messages.append({"role": "user", "content": f"NÃ¡sledujÃ­cÃ­ texty jsou relevantnÃ­ k dotazu:\n\n{context}\n\nOtÃ¡zka: {query}\nOdpovÄ›Ä:"})

    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=messages,
            max_tokens=1000,
            temperature=0.5,
            top_p=0.7
        )
        answer = response['choices'][0]['message']['content'].strip()
        st.session_state.history.append((query, answer))
        token_usage = response['usage']['total_tokens']
        prompt_tokens = response['usage']['prompt_tokens']
        completion_tokens = response['usage']['completion_tokens']
        save_results_to_file(query, answer, min_score, max_score, token_usage, prompt_tokens, completion_tokens, context)
        return answer
    except Exception as e:
        st.error(f"Chyba pÅ™i generovÃ¡nÃ­ odpovÄ›di: {e}")
        return "OmlouvÃ¡m se, doÅ¡lo k chybÄ› pÅ™i generovÃ¡nÃ­ odpovÄ›di."

def retrieve_and_respond(query, top_k=1):
    try:
        query_embedding = get_embedding(query)
        result = index.query(
            vector=query_embedding,
            top_k=top_k,
            include_metadata=True,
            filter={"metadata_field_name": {"$eq": "text_response"}}
        )
        matches = []
        for match in result.get('matches', []):
            score = match['score']
            if score >= 0.90:
                chunk_text = match['metadata'].get('chunk_text', 'No chunk text available')
                metadata_field_name = match['metadata'].get('metadata_field_name', 'N/A')
                matches.append({"score": score, "chunk_text": chunk_text, "metadata_field_name": metadata_field_name})
        if matches:
            best_match = matches[0]
            response = best_match['chunk_text']
            save_results_to_file(query, response, best_match['score'], best_match['score'], 0, 0, 0)
            return response
        retrieved_texts = retrieve_similar_texts(query, top_k=2)
        return generate_response(query, retrieved_texts)
    except Exception as e:
        st.error(f"Chyba pÅ™i vyhledÃ¡vÃ¡nÃ­ a odpovÃ­dÃ¡nÃ­: {e}")
        return "OmlouvÃ¡m se, doÅ¡lo k chybÄ› pÅ™i zpracovÃ¡nÃ­ vaÅ¡eho dotazu."

# Streamlit UI
st.title("TestovacÃ­ chatbot Fakulty informatiky a statistiky VÅ E v Praze")
st.write("""
Tento chatbot byl vytvoÅ™en v rÃ¡mci diplomovÃ© prÃ¡ce, kterÃ¡ se zamÄ›Å™uje na minimalizaci nÃ¡kladÅ¯ generativnÃ­ch dialogovÃ½ch systÃ©mÅ¯ pomocÃ­ pÅ™Ã­stupu Retrieval-Augmented Generation (RAG).

ğŸ¤– Chatbot je urÄen k odpovÃ­dÃ¡nÃ­ na dotazy tÃ½kajÃ­cÃ­ se informacÃ­ o studiu na FakultÄ› informatiky a statistiky VÅ E v Praze. 
Jeho znalosti jsou omezeny na pÅ™edem definovanÃ¡ tÃ©mata.

â— **Chatbot v tÃ©to demoverzi odpovÃ­dÃ¡ na otÃ¡zky v tÄ›chto oblastech:**

â€¢ ğŸ“ **StudijnÃ­ programy** â€“ bakalÃ¡Å™skÃ©, magisterskÃ©, doktorskÃ© a MBA  
â€¢ ğŸ“… **PoÅ¾adavky na pÅ™ijetÃ­** â€“ dokumenty, podmÃ­nky, termÃ­ny pÅ™ihlÃ¡Å¡ek  
â€¢ ğŸ“ **PÅ™ijÃ­macÃ­ Å™Ã­zenÃ­** â€“ prÅ¯bÄ›h zkouÅ¡ek, testy  
â€¢ ğŸŒ **ZahraniÄnÃ­ studenti** â€“ jazykovÃ© poÅ¾adavky, nostrifikace, vÃ­za  
â€¢ ğŸ’¸ **FinanÄnÃ­ zÃ¡leÅ¾itosti** â€“ Å¡kolnÃ©, stipendia a dalÅ¡Ã­ moÅ¾nosti podpory
""")

query = st.text_input("Zadejte dotaz a ovÄ›Å™te, jak si chatbot poradÃ­! ğŸ‘‡")

if query:
    with st.spinner("VyhledÃ¡vÃ¡nÃ­ relevantnÃ­ch textÅ¯..."):
        st.subheader("GenerovanÃ¡ odpovÄ›Ä:")
        response = retrieve_and_respond(query)
        st.write(response)

# StahovÃ¡nÃ­ logÅ¯ jako TXT
if st.session_state.log:
    st.download_button(
        label="ğŸ“„ StÃ¡hnout log jako TXT",
        data=io.StringIO(st.session_state.log),
        file_name="chatbot_log.txt",
        mime="text/plain"
    )

if st.session_state.unanswered_log:
    st.download_button(
        label="â“ StÃ¡hnout nezodpovÄ›zenÃ© dotazy",
        data=io.StringIO(st.session_state.unanswered_log),
        file_name="unanswered_log.txt",
        mime="text/plain"
    )









